{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KvswxQErGOSD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(os.getcwd())\n",
    "root = Path(path.parent.absolute())\n",
    "\n",
    "model_pre_path = root / 'Models' / 'Pretrained Inception'\n",
    "model_re_path = root / 'Models' / 'Retrained Inception'\n",
    "fetching_path = root / 'Shared Preprocessed Objects'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/nicol/Documents/Mestrado/Mat√©rias/Applications of Deep Learning/Pull Request - Rename/Models/Retrained Inception/HistoryArray.npy')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_re_path / 'HistoryArray.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5WptztJTGVkV"
   },
   "outputs": [],
   "source": [
    "#history_re = np.load(model_re_path / 'HistoryArray.npy', allow_pickle = True)\n",
    "#history_pre = np.load(model_pre_path / 'HistoryArray.npy', allow_pickle = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-DJ1pMBkGvPC"
   },
   "outputs": [],
   "source": [
    "def best_epoch(history):\n",
    "  best_epch = 0\n",
    "  best_loss = 10000\n",
    "  for epoch in range(int(len(history)/32)):\n",
    "    loss = 0\n",
    "    tmp = history[epoch*32:(epoch+1)*32]\n",
    "    #print(len(tmp))\n",
    "    for element in tmp:\n",
    "      loss += element.history['val_loss'][0]\n",
    "    loss /= 32\n",
    "    if loss < best_loss:\n",
    "      best_loss = loss\n",
    "      best_epoch = epoch\n",
    "    \n",
    "  return best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "TBKo0CToIKS6",
    "outputId": "476a93af-94fc-4e24-806b-4895a95c1297"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_re' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6100/4102088895.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mepoch_re\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_re\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mepoch_pre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_pre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history_re' is not defined"
     ]
    }
   ],
   "source": [
    "epoch_re = best_epoch(history_re)\n",
    "epoch_pre = best_epoch(history_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "q-JAhNGsJCvO"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model_pre = tf.keras.models.load_model(model_pre_path / 'TrainedLSTM' / ('Epoch=' + str(1)))\n",
    "model_re = tf.keras.models.load_model(model_re_path / 'TrainedLSTM' / ('Epoch=' + str(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CoI8n5mVJ9P7"
   },
   "outputs": [],
   "source": [
    "#target_path =  '/content/drive/MyDrive/Applications of Deep Learning - Medical Image Captioning/Models_V2/Pretrained Inception/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gJefkKbQNFtN"
   },
   "outputs": [],
   "source": [
    "# For pretrained model\n",
    "word2Index = np.load(fetching_path / \"word2Index.npy\", allow_pickle=True).item()\n",
    "index2Word = np.load(fetching_path / \"index2Word.npy\", allow_pickle=True).item()\n",
    "variable_params = np.load(fetching_path / \"variable_params.npy\", allow_pickle=True).item()\n",
    "test_captions = np.load(fetching_path / \"test_captions.npy\", allow_pickle=True).item()\n",
    "\n",
    "test_features_pre = np.load(model_pre_path / \"test_features_full.npy\", allow_pickle=True).item()\n",
    "test_features_re = np.load(model_re_path / \"test_features_full.npy\", allow_pickle=True).item()\n",
    "print(len(test_features_pre), len(test_features_re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "44NYQpXWPnjG"
   },
   "outputs": [],
   "source": [
    "keys_pre = list(test_features_pre.keys())\n",
    "values_pre = np.array(list(test_features_pre.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2FpKCTh-QF3M"
   },
   "outputs": [],
   "source": [
    "keys_re = list(test_features_re.keys())\n",
    "values_re = np.array(list(test_features_re.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "N4rMrq6YUuxi"
   },
   "outputs": [],
   "source": [
    "def greedySearch(model, image_features, max_len, word2Index, index2Word):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_len):\n",
    "        sequence = [word2Index[w] for w in in_text.split() if w in word2Index]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_len, padding = 'post')\n",
    "        \n",
    "        y_pred = model.predict([image_features,sequence], verbose=0)\n",
    "        y_pred = np.argmax(y_pred)\n",
    "        \n",
    "        word = index2Word[y_pred]\n",
    "        in_text += ' ' + word\n",
    "        \n",
    "        if word == 'endseq':\n",
    "            break\n",
    "\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "jR7Zc1gqUuvk"
   },
   "outputs": [],
   "source": [
    "def beam_search_predictions(model, image_features, max_len, word2Index, index2Word, beam_index):\n",
    "    start = [word2Index[\"startseq\"]]\n",
    "    start_word = [[start, 1]]\n",
    "    \n",
    "    final_preds = []\n",
    "    live_seqs = beam_index\n",
    "    image_features = np.tile(image_features, (beam_index,1))\n",
    "    errors = 0\n",
    "    count = 0\n",
    "    while len(start_word) > 0:\n",
    "        #print(count)\n",
    "        count+=1\n",
    "        temp = []\n",
    "        padded_seqs = []\n",
    "        #Get padded seqs for each of the starting seqs so far, misnamed as start_word\n",
    "        for s in start_word:\n",
    "            par_caps = pad_sequences([s[0]], maxlen=max_len, padding='post')\n",
    "            padded_seqs.append(par_caps)\n",
    "        \n",
    "        #Formatting input so that it can be used for a prediction\n",
    "        padded_seqs = np.array(padded_seqs).reshape(len(start_word), max_len)\n",
    "        \n",
    "        preds = model.predict([image_features[:len(start_word)],padded_seqs], verbose=0)\n",
    "        \n",
    "        #Getting the best branches for each of the start seqs that we had\n",
    "        for index, pred in enumerate(preds):\n",
    "            word_preds = np.argsort(pred)[-live_seqs:]\n",
    "            for w in word_preds:\n",
    "                next_cap, prob = start_word[index][0][:], start_word[index][1]\n",
    "                next_cap.append(w)\n",
    "                prob *= pred[w]\n",
    "                temp.append([next_cap, prob])\n",
    "                \n",
    "        start_word = temp\n",
    "        # Sorting according to the probabilities\n",
    "        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
    "        # Getting the top words from all branches\n",
    "        start_word = start_word[-live_seqs:]\n",
    "        \n",
    "        for pair in start_word:\n",
    "            #print(pair)\n",
    "            #pair[0] = list(map(lambda x: x.replace(0, 12), pair[0]))\n",
    "            errors += pair[0].count(0)\n",
    "            pair[0] = [12 if x==0 else x for x in pair[0]]\n",
    "            if index2Word[pair[0][-1]] == 'endseq':\n",
    "                final_preds.append([pair[0][:-1], pair[1]])\n",
    "                start_word = start_word[:-1]\n",
    "                live_seqs -= 1\n",
    "            if len(pair[0]) == max_len:\n",
    "                final_preds.append(pair)\n",
    "                start_word = start_word[:-1]\n",
    "                live_seqs -= 1\n",
    "    \n",
    "    # Between all the finished sequences (either max len or predicted endseq), decide which is best\n",
    "    max_prob = 0\n",
    "    for index, pred in enumerate(final_preds):\n",
    "        if pred[1] > max_prob:\n",
    "            best_index = index\n",
    "            max_prob = pred[1]\n",
    "    \n",
    "    # Convert to readable text\n",
    "    final_pred = final_preds[best_index]\n",
    "    final_caption = [index2Word[i] for i in final_pred[0]]\n",
    "    final_caption = ' '.join(final_caption[1:])\n",
    "    return final_caption, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gI4SFUIsQUAA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_prediction_single_image(model, image_features, max_len, word2Index, index2Word, method, beam_index = None): \n",
    "    if method == 'greedy':\n",
    "        return greedySearch(model, image_features, max_len, word2Index, index2Word)\n",
    "    else:\n",
    "        if beam_index == None:\n",
    "            print(\"For beam search you must input a beam_index. this is any natural number\")\n",
    "        else:\n",
    "            return beam_search_predictions(model, image_features, max_len, word2Index, index2Word, beam_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def generate_captions(model, values, model_path, variable_params, word2Index, index2Word, method, beam_index):\n",
    "    true_captions = {}\n",
    "    predicted_captions = {}\n",
    "\n",
    "    for index, features in enumerate(values_re):\n",
    "\n",
    "        predicted_caption = generate_prediction_single_image(model_re, features, \n",
    "                                     variable_params['max_caption_len'], word2Index, \n",
    "                                     index2Word, 'beam', beam_index = 5)\n",
    "\n",
    "\n",
    "        true_caption = test_captions[keys_re[index]].split()[1:-1]\n",
    "        true_caption = ' '.join(true_caption)\n",
    "\n",
    "        true_captions[keys_re[index]] = true_caption\n",
    "        predicted_captions[keys_re[index]] = predicted_caption\n",
    "        print(index, predicted_caption)\n",
    "        if index % 10 == 0:\n",
    "          tmp = predicted_captions\n",
    "          np.save(model_path / 'Predicted Captions', predicted_captions, allow_pickle = True)\n",
    "\n",
    "    np.save(model_path / 'Predicted Captions', predicted_captions, allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28936/4062713050.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenerate_captions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_re\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues_re\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_re_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2Index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex2Word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'beam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeam_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28936/1344690246.py\u001b[0m in \u001b[0;36mgenerate_captions\u001b[1;34m(model, values, model_path, variable_params, word2Index, index2Word, method, beam_index)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues_re\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         predicted_caption = generate_prediction_single_image(model_re, features, \n\u001b[0m\u001b[0;32m     10\u001b[0m                                      \u001b[0mvariable_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_caption_len'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2Index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                      index2Word, 'beam', beam_index = 5)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28936/2874065588.py\u001b[0m in \u001b[0;36mgenerate_prediction_single_image\u001b[1;34m(model, image_features, max_len, word2Index, index2Word, method, beam_index)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;31m#print(\"Beam Search, K = \" + str(beam_index) + \":\", beam_search_predictions(model, image_features, max_len, word2Index, index2Word, beam_index))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mbeam_search_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2Index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex2Word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeam_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28936/2065374008.py\u001b[0m in \u001b[0;36mbeam_search_predictions\u001b[1;34m(model, image_features, max_len, word2Index, index2Word, beam_index)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mpadded_seqs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded_seqs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadded_seqs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m#Getting the best branches for each of the start seqs that we had\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1745\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1746\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1747\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Single epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1748\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    694\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    717\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 719\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3117\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3118\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3119\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   3120\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0;32m   3121\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate captions for retrained model\n",
    "generate_captions(model_re, values_re, model_re_path, variable_params, word2Index, index2Word, 'beam', beam_index = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "#Generate captions for pretrained model\n",
    "generate_captions(model_pre, values_pre, model_pre_path, variable_params, word2Index, index2Word, 'beam', beam_index = 2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
