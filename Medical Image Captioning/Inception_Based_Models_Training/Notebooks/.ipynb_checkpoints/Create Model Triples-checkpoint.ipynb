{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b60fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a171c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(os.getcwd())\n",
    "root = Path(path.parent.absolute())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2e512a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path =  'D://Mestrado//Applications of Deep Learning//Compartmentalized_Models//Retrained_Imagenet//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3567701",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''word2Index = np.load(target_path + \"word2Index.npy\", allow_pickle=True).item()\n",
    "variable_params = np.load(target_path + \"variable_params.npy\", allow_pickle=True).item()\n",
    "\n",
    "train_features = np.load(target_path + \"train_features_full.npy\", allow_pickle=True).item()\n",
    "test_features = np.load(target_path + \"test_features_full.npy\", allow_pickle=True).item()\n",
    "\n",
    "train_captions = np.load(target_path + \"train_captions.npy\", allow_pickle=True).item()\n",
    "test_captions = np.load(target_path + \"test_captions.npy\", allow_pickle=True).item()'''\n",
    "\n",
    "word2Index = np.load(target_path + \"word2Index.npy\", allow_pickle=True).item()\n",
    "variable_params = np.load(target_path + \"variable_params.npy\", allow_pickle=True).item()\n",
    "\n",
    "train_features = np.load(target_path + \"Retrained_Inception_Features_Train.npy\", allow_pickle=True).item()\n",
    "test_features = np.load(target_path + \"Retrained_Inception_Features_Test.npy\", allow_pickle=True).item()\n",
    "\n",
    "train_captions = np.load(target_path + \"train_captions.npy\", allow_pickle=True).item()\n",
    "test_captions = np.load(target_path + \"test_captions.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(target_path + 'training_triples')\n",
    "os.mkdir(target_path + 'test_triples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36790b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_batch(X1, X2, y, path, batch):\n",
    "    np.save(path + 'features_batch_' + str(batch), np.array(X1))\n",
    "    np.save(path + 'captions_batch_' + str(batch), np.array(X2))\n",
    "    np.save(path + 'outputs_batch_' + str(batch), np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17841eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "def save_LSTM_triples_batches(path, features, captions, word2Index, max_len, vocab_size, batch_size = 2048, show_progress = True):\n",
    "    ''' This function generates the triples (feature, caption up until a certain word, next word of the true caption) that will be used to train the LSTM model. may take a minute to run.\n",
    "    If batch_size > len(features) then it will generate only one batch. This is not very advisable, however, because this may cause OOM problems during training and even in this function.'''\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    y  = []\n",
    "\n",
    "    batch = 0\n",
    "    count = 0\n",
    "    for key in features:\n",
    "        count += 1\n",
    "        # Fetching features and captions\n",
    "        image_features = features[key]\n",
    "        caption = captions[key]\n",
    "        \n",
    "        # Encoding caption\n",
    "        seq = [word2Index[word] for word in caption.split(' ') if word in word2Index]\n",
    "        \n",
    "        # Splitting encoded sequence into X,y pair\n",
    "        for i in range(1, len(seq)):\n",
    "            # input-output pair split\n",
    "            input_seq, output_seq = seq[:i], seq[i]\n",
    "            # padding input sequence\n",
    "            input_seq = pad_sequences([input_seq], maxlen=max_len, padding = 'post')[0]\n",
    "            # encoding output Sequence\n",
    "            output_seq = to_categorical([output_seq], num_classes=vocab_size)[0]\n",
    "            \n",
    "            # appending and storage\n",
    "            X1.append(image_features)\n",
    "            X2.append(input_seq)\n",
    "            y.append(output_seq)\n",
    "\n",
    "        if count % batch_size == 0:\n",
    "            # Save current batch\n",
    "            save_batch(X1, X2, y, path, batch)\n",
    "            \n",
    "            # Reset Batch\n",
    "            X1 = []\n",
    "            X2 = []\n",
    "            y  = []\n",
    "            gc.collect()\n",
    "            \n",
    "            batch += 1\n",
    "            print('Treated ' + str(batch_size*batch) + ' images out of ' + str(len(features)))\n",
    "\n",
    "    save_batch(X1, X2, y, path, batch)\n",
    "    \n",
    "    return batch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f8c3943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treated 2048 images out of 65423\n",
      "Treated 4096 images out of 65423\n",
      "Treated 6144 images out of 65423\n",
      "Treated 8192 images out of 65423\n",
      "Treated 10240 images out of 65423\n",
      "Treated 12288 images out of 65423\n",
      "Treated 14336 images out of 65423\n",
      "Treated 16384 images out of 65423\n",
      "Treated 18432 images out of 65423\n",
      "Treated 20480 images out of 65423\n",
      "Treated 22528 images out of 65423\n",
      "Treated 24576 images out of 65423\n",
      "Treated 26624 images out of 65423\n",
      "Treated 28672 images out of 65423\n",
      "Treated 30720 images out of 65423\n",
      "Treated 32768 images out of 65423\n",
      "Treated 34816 images out of 65423\n",
      "Treated 36864 images out of 65423\n",
      "Treated 38912 images out of 65423\n",
      "Treated 40960 images out of 65423\n",
      "Treated 43008 images out of 65423\n",
      "Treated 45056 images out of 65423\n",
      "Treated 47104 images out of 65423\n",
      "Treated 49152 images out of 65423\n",
      "Treated 51200 images out of 65423\n",
      "Treated 53248 images out of 65423\n",
      "Treated 55296 images out of 65423\n",
      "Treated 57344 images out of 65423\n",
      "Treated 59392 images out of 65423\n",
      "Treated 61440 images out of 65423\n",
      "Treated 63488 images out of 65423\n",
      "Treated 2048 images out of 8176\n",
      "Treated 4096 images out of 8176\n",
      "Treated 6144 images out of 8176\n"
     ]
    }
   ],
   "source": [
    "# Also takes a long time. Around 30 minutes \n",
    "# Saving training triples\n",
    "num_batches_training = save_LSTM_triples_batches(target_path + 'training_triples//', train_features, \n",
    "                                   train_captions, word2Index, variable_params['max_caption_len'], \n",
    "                                   variable_params['vocab_size'], batch_size = 2048)\n",
    "\n",
    "# Saving test triples. I am not sure this is required.\n",
    "num_batches_test = save_LSTM_triples_batches(target_path + 'test_triples//', test_features, \n",
    "                                   test_captions, word2Index, variable_params['max_caption_len'], \n",
    "                                   variable_params['vocab_size'], batch_size = 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8696e821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
